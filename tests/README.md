# EEG-ExPy Integration Tests

This directory contains integration tests for the EEG-ExPy experiment framework, with a focus on high-coverage testing of the N170 visual experiment.

## Overview

The test suite provides comprehensive integration testing with mocked EEG devices and PsychoPy components for headless testing in CI/CD environments. The tests verify complete experiment workflows including initialization, stimulus presentation, EEG integration, controller input handling, and error scenarios.

## Directory Structure

```
tests/
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ conftest.py                            # Shared pytest fixtures and mock classes
â”œâ”€â”€ fixtures/                              # Additional test fixtures (future)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ integration/                           # Integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_n170_integration.py          # N170 experiment tests
â”œâ”€â”€ test_empty.py                          # Placeholder test
â””â”€â”€ test_run_experiments.py                # Manual integration test (not run in CI)
```

## Test Architecture

### Mock Infrastructure (conftest.py)

The test suite uses custom mock classes that simulate the behavior of real hardware and UI components:

#### **MockEEG**
Simulates the `eegnb.devices.eeg.EEG` interface:
- Tracks start/stop calls
- Records marker pushes with timestamps
- Provides synthetic EEG data
- Configurable for different device types (Muse2, Ganglion, Cyton, etc.)

```python
def test_example(mock_eeg):
    mock_eeg.start("/tmp/recording", duration=10)
    mock_eeg.push_sample(marker=[1], timestamp=1.5)
    assert len(mock_eeg.markers) == 1
```

#### **MockWindow**
Simulates PsychoPy Window for headless testing:
- No display required
- Tracks flip() calls
- Supports context manager protocol

#### **MockImageStim / MockTextStim**
Simulates PsychoPy visual stimuli:
- Tracks draw() calls
- Supports image/text updates
- Lightweight for fast testing

#### **MockClock**
Provides deterministic timing control:
- Manual time advancement
- Predictable timestamps for testing

### Fixtures

#### Core Fixtures

- **`mock_eeg`**: Fresh MockEEG instance for each test
- **`mock_eeg_muse2`**: Muse2-specific configuration
- **`temp_save_fn(tmp_path)`**: Temporary file path for recordings
- **`mock_psychopy(mocker)`**: Complete PsychoPy mock setup
- **`mock_psychopy_with_spacebar`**: Auto-starts experiments with spacebar
- **`mock_vr_disabled`**: Disables VR controller input
- **`mock_vr_button_press`**: Simulates VR button press
- **`stimulus_images(tmp_path)`**: Creates temporary stimulus directory

#### Using Fixtures

```python
def test_experiment_with_eeg(mock_eeg, temp_save_fn, mock_psychopy):
    experiment = VisualN170(
        duration=10,
        eeg=mock_eeg,
        save_fn=temp_save_fn,
        use_vr=False
    )
    experiment.run(instructions=False)
    assert mock_eeg.start_count > 0
```

## N170 Integration Tests

The N170 test suite (`test_n170_integration.py`) contains **10 minimal, high-value tests** organized into 5 focused test classes.

**All tests follow the normal initialization flow**: `__init__()` â†’ `setup()` â†’ `run()`

### Test Coverage

This minimal test suite provides maximum value with minimum test count:

#### âœ… TestN170Core (4 tests)
**Critical path testing:**
- Basic initialization with all parameters
- Setup creates window and loads stimuli properly
- Full experiment run with EEG device (end-to-end)
- Full experiment run without EEG device (end-to-end)

#### âœ… TestN170DeviceIntegration (1 test)
**Hardware integration:**
- Device initialization and setup (Muse2 example)

#### âœ… TestN170EdgeCases (2 tests)
**Boundary conditions:**
- Zero trials edge case
- Minimal timing configuration

#### âœ… TestN170UserInteraction (2 tests)
**User input handling:**
- Keyboard input (spacebar start, escape cancel)
- VR mode initialization

#### âœ… TestN170SaveFunction (1 test)
**File handling:**
- Save function integration with generate_save_fn()

### Current Status

- **âœ… 10/10 tests passing (100%)**
- **Test execution time: ~3.6 seconds**
- **Coverage: ~69% of n170.py module**

### Design Philosophy

This test suite follows the **minimal viable testing** approach:
- Each test provides unique, high-value coverage
- No redundant or low-value tests
- Fast execution for rapid development feedback
- Focus on critical paths and integration points
- All tests use proper initialization flow

## Running Tests

### Run All Integration Tests

```bash
pytest tests/integration/
```

### Run N170 Tests Only

```bash
pytest tests/integration/test_n170_integration.py
```

### Run Specific Test Class

```bash
pytest tests/integration/test_n170_integration.py::TestN170Initialization
```

### Run With Coverage Report

```bash
pytest tests/integration/ --cov=eegnb --cov-report=html
```

### Run Fast Tests Only (Skip Slow Tests)

```bash
pytest tests/integration/ -m "not slow"
```

### Verbose Output

```bash
pytest tests/integration/ -v
```

### Show Test Names Without Running

```bash
pytest tests/integration/ --collect-only
```

## Test Markers

Tests can be marked with pytest markers for selective execution:

- `@pytest.mark.integration`: Integration test (all tests in this suite)
- `@pytest.mark.slow`: Slow-running test (skip in quick test runs)
- `@pytest.mark.requires_display`: Requires display (currently none)

## Dependencies

### Required Packages

```bash
pip install pytest pytest-cov pytest-mock numpy
```

### Optional (for full experiment functionality)

```bash
pip install -r requirements.txt
```

The test suite is designed to work with minimal dependencies by mocking heavy dependencies like PsychoPy, BrainFlow, and MuseLSL.

## CI/CD Integration

The tests are designed to run automatically in GitHub Actions with:
- **Ubuntu 22.04, Windows, macOS** support
- **Headless display** via Xvfb on Linux
- **Python 3.8, 3.10** compatibility
- **Automatic coverage reporting**

### Branch Triggers

Tests run automatically on push to:
- `master` - Production branch
- `develop` - Development branch
- `dev/*` - Feature development branches
- `claude/*` - AI-assisted development branches (NEW)

### GitHub Actions Configuration

The workflow is configured in `.github/workflows/test.yml`:

```yaml
on:
  push:
    branches: [ master, develop, 'dev/*', 'claude/*' ]
  pull_request:
    branches: [ master, develop ]
```

**Test execution:**
```yaml
- name: Run examples with coverage
  run: |
    if [ "$RUNNER_OS" == "Linux" ]; then
      Xvfb :0 -screen 0 1024x768x24 -ac +extension GLX +render -noreset &> xvfb.log &
      export DISPLAY=:0
    fi
    make test PYTEST_ARGS="--ignore=tests/test_run_experiments.py"
```

This ensures all tests run in a headless environment on Linux while still using the display server for PsychoPy components.

## Writing New Tests

### Basic Test Template

```python
@pytest.mark.integration
class TestNewFeature:
    """Test description."""

    def test_basic_functionality(self, mock_eeg, temp_save_fn, mock_psychopy):
        """Test basic functionality."""
        # Arrange
        experiment = VisualN170(
            duration=5,
            eeg=mock_eeg,
            save_fn=temp_save_fn,
            use_vr=False
        )

        # Act
        result = experiment.some_method()

        # Assert
        assert result is not None
        assert mock_eeg.start_count > 0
```

### Parametrized Test Template

```python
@pytest.mark.parametrize("duration,n_trials", [
    (5, 10),
    (10, 20),
    (15, 30),
])
def test_various_configurations(mock_eeg, temp_save_fn, duration, n_trials):
    """Test with various configurations."""
    experiment = VisualN170(
        duration=duration,
        eeg=mock_eeg,
        save_fn=temp_save_fn,
        n_trials=n_trials
    )
    assert experiment.duration == duration
    assert experiment.n_trials == n_trials
```

## Best Practices

### 1. Use Fixtures for Reusable Components

```python
@pytest.fixture
def configured_experiment(mock_eeg, temp_save_fn):
    return VisualN170(
        duration=10,
        eeg=mock_eeg,
        save_fn=temp_save_fn,
        n_trials=5
    )

def test_with_fixture(configured_experiment):
    assert configured_experiment.duration == 10
```

### 2. Mock at the Right Level

- Mock external dependencies (PsychoPy, BrainFlow)
- Don't mock the code you're testing
- Use `mocker.patch()` for temporary mocks in specific tests

### 3. Test Behavior, Not Implementation

```python
# Good: Test observable behavior
def test_markers_are_recorded(mock_eeg):
    experiment.present_stimulus(0)
    assert len(mock_eeg.markers) > 0

# Avoid: Testing internal implementation details
def test_internal_variable_name(experiment):
    assert hasattr(experiment, '_internal_var')  # Fragile
```

### 4. Use Descriptive Test Names

```python
# Good
def test_experiment_starts_eeg_device_when_run():
    pass

# Avoid
def test_run():
    pass
```

### 5. Keep Tests Independent

Each test should:
- Set up its own state
- Not depend on other tests
- Clean up after itself (handled by fixtures)

### 6. Test Edge Cases

```python
def test_zero_trials(mock_eeg, temp_save_fn):
    """Test handling of edge case: zero trials."""
    experiment = VisualN170(
        duration=10,
        eeg=mock_eeg,
        save_fn=temp_save_fn,
        n_trials=0  # Edge case
    )
    assert experiment.n_trials == 0
```

## Troubleshooting

### Import Errors

If you see import errors for PsychoPy, BrainFlow, etc.:
```python
# These are mocked at module level in test files
import sys
from unittest.mock import MagicMock
sys.modules['psychopy'] = MagicMock()
```

### Fixture Not Found

Ensure conftest.py is in the tests/ directory and fixtures are properly defined.

### Tests Pass Locally But Fail in CI

- Check for hardcoded paths
- Ensure tests don't require display
- Verify all dependencies are in requirements.txt

### Timeout Errors

For slow tests:
```python
@pytest.mark.timeout(60)  # 60 second timeout
def test_slow_operation():
    pass
```

## Coverage Goals

Current coverage for N170 module: **~69%**

Target coverage goals:
- **Critical paths**: 90%+ (initialization, EEG integration)
- **Overall module**: 80%+
- **Edge cases**: 70%+

View coverage report:
```bash
pytest --cov=eegnb.experiments.visual_n170 --cov-report=html
open htmlcov/index.html
```

## Future Enhancements

### Planned Improvements

1. **Complete Stimulus Loading Tests**
   - Mock stimulus file loading
   - Test with actual small test images

2. **Add More Experiment Types**
   - P300 integration tests
   - SSVEP integration tests
   - Auditory oddball tests

3. **Performance Benchmarking**
   - Time critical operations
   - Memory usage tracking
   - Frame rate validation

4. **Real Hardware Integration**
   - Optional tests with synthetic EEG device
   - BrainFlow synthetic board integration

5. **Visual Regression Testing**
   - Capture and compare stimulus rendering
   - Ensure UI consistency

## Contributing

When adding new tests:

1. Place tests in appropriate test class or create new class
2. Use existing fixtures when possible
3. Add docstrings to all test methods
4. Mark slow tests with `@pytest.mark.slow`
5. Update this README with new test categories
6. Ensure tests pass locally before committing

## Resources

- [pytest documentation](https://docs.pytest.org/)
- [pytest-mock documentation](https://pytest-mock.readthedocs.io/)
- [unittest.mock guide](https://docs.python.org/3/library/unittest.mock.html)
- [EEG-ExPy documentation](https://neurotechx.github.io/eeg-notebooks/)

## Questions or Issues?

- Open an issue on GitHub
- Check existing tests for examples
- Review conftest.py for available fixtures
- Consult the pytest documentation

---

**Test Suite Status**: ðŸŸ¢ Operational (10/10 tests passing - 100%)
**Test Execution Time**: ~3.6 seconds
**Last Updated**: 2025-11-05
**Maintainer**: EEG-ExPy Team
**CI/CD**: Runs automatically on `master`, `develop`, `dev/*`, and `claude/*` branches
**Note**: Minimal viable test suite with maximum value coverage
